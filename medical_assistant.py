#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒ»ç–—åŠ©æ‰‹é›†æˆè„šæœ¬
åŸºäº Qwen3-1.7B åŒ»ç–—å¾®è°ƒæ¨¡å‹ï¼Œæä¾›å¤šç§åŒ»ç–—åœºæ™¯çš„æ™ºèƒ½åŠ©æ‰‹åŠŸèƒ½
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import argparse
import json
import time
from datetime import datetime
import os

# åŒ»ç–—ä¸“ä¸šæç¤ºè¯æ¨¡æ¿
MEDICAL_PROMPTS = {
    "diagnosis": "ä½ æ˜¯ä¸€åç»éªŒä¸°å¯Œçš„ä¸´åºŠåŒ»ç”Ÿï¼Œè¯·æ ¹æ®æ‚£è€…æè¿°çš„ç—‡çŠ¶ï¼Œç»™å‡ºä¸“ä¸šçš„åˆæ­¥è¯Šæ–­å»ºè®®å’Œè¿›ä¸€æ­¥æ£€æŸ¥å»ºè®®ã€‚",
    "treatment": "ä½ æ˜¯ä¸€ååŒ»ç”Ÿï¼Œè¯·æ ¹æ®ç—…æƒ…æè¿°ï¼Œæä¾›ä¸“ä¸šçš„æ²»ç–—æ–¹æ¡ˆå»ºè®®ï¼ŒåŒ…æ‹¬ç”¨è¯æŒ‡å¯¼å’Œæ³¨æ„äº‹é¡¹ã€‚",
    "prevention": "ä½ æ˜¯ä¸€åé¢„é˜²åŒ»å­¦ä¸“å®¶ï¼Œè¯·æä¾›ä¸“ä¸šçš„ç–¾ç—…é¢„é˜²å»ºè®®å’Œå¥åº·ç”Ÿæ´»æ–¹å¼æŒ‡å¯¼ã€‚",
    "education": "ä½ æ˜¯ä¸€ååŒ»å­¦æ•™è‚²ä¸“å®¶ï¼Œè¯·ç”¨é€šä¿—æ˜“æ‡‚çš„æ–¹å¼è§£é‡ŠåŒ»å­¦æ¦‚å¿µï¼Œå¸®åŠ©æ‚£è€…ç†è§£ç–¾ç—…ç›¸å…³çŸ¥è¯†ã€‚",
    "emergency": "ä½ æ˜¯ä¸€åæ€¥è¯Šç§‘åŒ»ç”Ÿï¼Œè¯·è¯„ä¼°ç—‡çŠ¶çš„ç´§æ€¥ç¨‹åº¦ï¼Œç»™å‡ºæ˜¯å¦éœ€è¦ç«‹å³å°±åŒ»çš„å»ºè®®ã€‚",
    "nutrition": "ä½ æ˜¯ä¸€åè¥å…»å¸ˆï¼Œè¯·æ ¹æ®æ‚£è€…çš„å¥åº·çŠ¶å†µï¼Œæä¾›ä¸“ä¸šçš„è¥å…»å»ºè®®å’Œé¥®é£ŸæŒ‡å¯¼ã€‚",
    "mental_health": "ä½ æ˜¯ä¸€åå¿ƒç†åŒ»ç”Ÿï¼Œè¯·å…³æ³¨æ‚£è€…çš„å¿ƒç†å¥åº·çŠ¶å†µï¼Œæä¾›ä¸“ä¸šçš„å¿ƒç†æ”¯æŒå’Œå»ºè®®ã€‚",
    "pediatric": "ä½ æ˜¯ä¸€åå„¿ç§‘åŒ»ç”Ÿï¼Œè¯·æ ¹æ®å„¿ç«¥çš„ç‰¹æ®Šæƒ…å†µï¼Œæä¾›é€‚åˆçš„åŒ»ç–—å»ºè®®å’ŒæŠ¤ç†æŒ‡å¯¼ã€‚",
    "geriatric": "ä½ æ˜¯ä¸€åè€å¹´åŒ»å­¦ä¸“å®¶ï¼Œè¯·è€ƒè™‘è€å¹´äººçš„ç‰¹æ®Šéœ€æ±‚ï¼Œæä¾›é€‚åˆçš„åŒ»ç–—å»ºè®®ã€‚",
    "women_health": "ä½ æ˜¯ä¸€åå¦‡ç§‘åŒ»ç”Ÿï¼Œè¯·ä¸ºå¥³æ€§æ‚£è€…æä¾›ä¸“ä¸šçš„å¥åº·å»ºè®®å’ŒåŒ»ç–—æŒ‡å¯¼ã€‚",
}

# å¸¸è§åŒ»ç–—åœºæ™¯
MEDICAL_SCENARIOS = {
    "1": "ç—‡çŠ¶è¯Šæ–­",
    "2": "æ²»ç–—æ–¹æ¡ˆ",
    "3": "ç–¾ç—…é¢„é˜²",
    "4": "åŒ»å­¦æ•™è‚²",
    "5": "ç´§æ€¥è¯„ä¼°",
    "6": "è¥å…»æŒ‡å¯¼",
    "7": "å¿ƒç†å¥åº·",
    "8": "å„¿ç§‘å’¨è¯¢",
    "9": "è€å¹´å¥åº·",
    "10": "å¥³æ€§å¥åº·",
}

# é¢„è®¾é—®é¢˜ç¤ºä¾‹
SAMPLE_QUESTIONS = {
    "diagnosis": [
        "æˆ‘æœ€è¿‘ç»å¸¸å¤´ç—›ï¼Œä¼´æœ‰æ¶å¿ƒï¼Œè¿™æ˜¯ä»€ä¹ˆåŸå› ï¼Ÿ",
        "èƒ¸ç—›æŒç»­äº†3å¤©ï¼Œå‘¼å¸æ—¶åŠ é‡ï¼Œå¯èƒ½æ˜¯ä»€ä¹ˆé—®é¢˜ï¼Ÿ",
        "æŒç»­å‘çƒ­ä¸€å‘¨ï¼Œä½“æ¸©åœ¨38-39åº¦ä¹‹é—´ï¼Œéœ€è¦åšä»€ä¹ˆæ£€æŸ¥ï¼Ÿ"
    ],
    "treatment": [
        "é«˜è¡€å‹æ‚£è€…åº”è¯¥å¦‚ä½•æ§åˆ¶è¡€å‹ï¼Ÿ",
        "ç³–å°¿ç—…æ‚£è€…é™¤äº†æ§åˆ¶è¡€ç³–ï¼Œè¿˜éœ€è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ",
        "æ„Ÿå†’æœŸé—´åº”è¯¥æ€ä¹ˆç”¨è¯ï¼Ÿ"
    ],
    "prevention": [
        "å¦‚ä½•é¢„é˜²å¿ƒè¡€ç®¡ç–¾ç—…ï¼Ÿ",
        "å†¬å­£å¦‚ä½•é¢„é˜²æ„Ÿå†’ï¼Ÿ",
        "å¦‚ä½•é¢„é˜²éª¨è´¨ç–æ¾ï¼Ÿ"
    ],
    "education": [
        "ä»€ä¹ˆæ˜¯é«˜è¡€å‹ï¼Ÿ",
        "ç³–å°¿ç—…çš„å‘ç—…æœºåˆ¶æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¿ƒè‚Œæ¢—æ­»æ˜¯å¦‚ä½•å‘ç”Ÿçš„ï¼Ÿ"
    ]
}

class MedicalAssistant:
    def __init__(self, checkpoint_path="./output/Qwen3-1.7B/checkpoint-1084"):
        """åˆå§‹åŒ–åŒ»ç–—åŠ©æ‰‹"""
        self.checkpoint_path = checkpoint_path
        self.device, self.dtype = self._select_device_and_dtype()
        self.model = None
        self.tokenizer = None
        self.conversation_history = []
        
    def _select_device_and_dtype(self):
        """é€‰æ‹©è®¾å¤‡å’Œæ•°æ®ç±»å‹"""
        if torch.cuda.is_available():
            try:
                major, _ = torch.cuda.get_device_capability()
                if major >= 12:
                    raise RuntimeError("Unsupported CUDA capability for current PyTorch")
                _ = torch.zeros(1, device="cuda")
                return "cuda", torch.float16
            except Exception:
                pass
        return "cpu", torch.float32
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print("æ­£åœ¨åŠ è½½åŒ»ç–—åŠ©æ‰‹æ¨¡å‹...")
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.checkpoint_path):
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.checkpoint_path}")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.checkpoint_path, 
            use_fast=False, 
            trust_remote_code=True,
            local_files_only=True  # åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        )
        if self.tokenizer.pad_token is None and self.tokenizer.eos_token is not None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.checkpoint_path, 
            torch_dtype=self.dtype,
            local_files_only=True  # åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        )
        self.model.to(self.device)
        self.model.eval()
        
        print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def predict(self, messages, max_new_tokens=4096):
        """æ‰§è¡Œé¢„æµ‹"""
        model_device = next(self.model.parameters()).device
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        inputs = self.tokenizer([text], return_tensors="pt")
        input_ids = inputs.input_ids.to(model_device)
        attention_mask = inputs.attention_mask.to(model_device) if hasattr(inputs, "attention_mask") else None

        generated = self.model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
        )

        # åªè§£ç æ–°ç”Ÿæˆéƒ¨åˆ†
        new_tokens = generated[:, input_ids.shape[1]:]
        response = self.tokenizer.batch_decode(new_tokens, skip_special_tokens=True)[0]
        return response
    
    def ask_question(self, question, scenario_type="diagnosis", max_tokens=4096):
        """è¯¢é—®åŒ»ç–—é—®é¢˜"""
        if scenario_type not in MEDICAL_PROMPTS:
            scenario_type = "diagnosis"
        
        messages = [
            {"role": "system", "content": MEDICAL_PROMPTS[scenario_type]},
            {"role": "user", "content": question}
        ]
        
        # è®°å½•å¯¹è¯å†å²
        self.conversation_history.append({
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "scenario": scenario_type,
            "question": question,
            "response": None
        })
        
        response = self.predict(messages, max_new_tokens=max_tokens)
        
        # æ›´æ–°å¯¹è¯å†å²
        self.conversation_history[-1]["response"] = response
        
        return response
    
    def show_scenarios(self):
        """æ˜¾ç¤ºå¯ç”¨çš„åŒ»ç–—åœºæ™¯"""
        print("\nğŸ¥ åŒ»ç–—åŠ©æ‰‹ - å¯ç”¨åœºæ™¯:")
        print("=" * 50)
        for key, value in MEDICAL_SCENARIOS.items():
            print(f"{key:2}. {value}")
        print("=" * 50)
    
    def show_sample_questions(self, scenario_type):
        """æ˜¾ç¤ºç¤ºä¾‹é—®é¢˜"""
        if scenario_type in SAMPLE_QUESTIONS:
            print(f"\nğŸ“‹ {MEDICAL_SCENARIOS.get(scenario_type, 'åŒ»ç–—å’¨è¯¢')} - ç¤ºä¾‹é—®é¢˜:")
            print("-" * 40)
            for i, question in enumerate(SAMPLE_QUESTIONS[scenario_type], 1):
                print(f"{i}. {question}")
            print("-" * 40)
    
    def interactive_mode(self):
        """äº¤äº’æ¨¡å¼"""
        print("\nğŸ¤– åŒ»ç–—åŠ©æ‰‹å·²å¯åŠ¨ï¼")
        print("è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©ï¼Œè¾“å…¥ 'quit' é€€å‡º")
        
        while True:
            try:
                # æ˜¾ç¤ºåœºæ™¯é€‰æ‹©
                self.show_scenarios()
                
                # é€‰æ‹©åœºæ™¯
                scenario_choice = input("\nè¯·é€‰æ‹©åŒ»ç–—åœºæ™¯ (1-10): ").strip()
                if scenario_choice == 'quit':
                    break
                elif scenario_choice == 'help':
                    self.show_help()
                    continue
                elif scenario_choice not in MEDICAL_SCENARIOS:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
                    continue
                
                # è·å–åœºæ™¯ç±»å‹
                scenario_type = list(MEDICAL_PROMPTS.keys())[int(scenario_choice) - 1]
                
                # æ˜¾ç¤ºç¤ºä¾‹é—®é¢˜
                self.show_sample_questions(scenario_type)
                
                # è·å–ç”¨æˆ·é—®é¢˜
                question = input(f"\nè¯·è¾“å…¥æ‚¨çš„{MEDICAL_SCENARIOS[scenario_choice]}é—®é¢˜: ").strip()
                if not question:
                    print("âŒ é—®é¢˜ä¸èƒ½ä¸ºç©º")
                    continue
                
                # ç”Ÿæˆå›ç­”
                print("\nğŸ”„ æ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜...")
                start_time = time.time()
                
                response = self.ask_question(question, scenario_type)
                
                end_time = time.time()
                
                # æ˜¾ç¤ºå›ç­”
                elapsed_time = end_time - start_time
                print(f"\nğŸ’¡ åŒ»ç–—åŠ©æ‰‹å›ç­” (è€—æ—¶: {elapsed_time:.2f}ç§’):")
                print("=" * 60)
                print(response)
                print("=" * 60)
                
                # è¯¢é—®æ˜¯å¦ç»§ç»­
                continue_choice = input("\næ˜¯å¦ç»§ç»­å’¨è¯¢ï¼Ÿ(y/n): ").strip().lower()
                if continue_choice in ['n', 'no', 'å¦']:
                    break
                    
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨åŒ»ç–—åŠ©æ‰‹ï¼")
                break
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
                continue
    
    def show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        print("\nğŸ“– åŒ»ç–—åŠ©æ‰‹ä½¿ç”¨å¸®åŠ©:")
        print("=" * 50)
        print("1. é€‰æ‹©åŒ»ç–—åœºæ™¯ (1-10)")
        print("2. è¾“å…¥æ‚¨çš„åŒ»ç–—é—®é¢˜")
        print("3. è·å¾—ä¸“ä¸šçš„åŒ»ç–—å»ºè®®")
        print("\nğŸ’¡ æç¤º:")
        print("- æœ¬åŠ©æ‰‹ä»…æä¾›å‚è€ƒå»ºè®®ï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­")
        print("- ç´§æ€¥æƒ…å†µè¯·ç«‹å³å°±åŒ»")
        print("- è¾“å…¥ 'quit' é€€å‡ºç¨‹åº")
        print("=" * 50)
    
    def save_conversation(self, filename=None):
        """ä¿å­˜å¯¹è¯å†å²"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"medical_conversation_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ å¯¹è¯å†å²å·²ä¿å­˜åˆ°: {filename}")
    
    def batch_questions(self, questions_file):
        """æ‰¹é‡å¤„ç†é—®é¢˜"""
        try:
            with open(questions_file, 'r', encoding='utf-8') as f:
                questions = json.load(f)
            
            print(f"ğŸ“ å¼€å§‹æ‰¹é‡å¤„ç† {len(questions)} ä¸ªé—®é¢˜...")
            
            results = []
            for i, q in enumerate(questions, 1):
                print(f"\nå¤„ç†ç¬¬ {i}/{len(questions)} ä¸ªé—®é¢˜...")
                response = self.ask_question(
                    q.get('question', ''), 
                    q.get('scenario', 'diagnosis'),
                    q.get('max_tokens', 4096)
                )
                
                results.append({
                    "question": q.get('question', ''),
                    "scenario": q.get('scenario', 'diagnosis'),
                    "response": response
                })
            
            # ä¿å­˜ç»“æœ
            output_file = f"batch_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")


def main():
    parser = argparse.ArgumentParser(description="åŒ»ç–—åŠ©æ‰‹ - åŸºäºQwen3-1.7Bçš„æ™ºèƒ½åŒ»ç–—å’¨è¯¢ç³»ç»Ÿ")
    parser.add_argument("--checkpoint", "-c", type=str, 
                       default="./models/Qwen/Qwen3-1___7B",
                       help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--question", "-q", type=str, 
                       help="ç›´æ¥è¯¢é—®é—®é¢˜ï¼ˆéœ€è¦é…åˆ --scenario ä½¿ç”¨ï¼‰")
    parser.add_argument("--scenario", "-s", type=str, 
                       default="diagnosis", 
                       choices=list(MEDICAL_PROMPTS.keys()),
                       help="åŒ»ç–—åœºæ™¯ç±»å‹")
    parser.add_argument("--max-tokens", "-m", type=int, 
                       default=4096,
                       help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--batch", "-b", type=str, 
                       help="æ‰¹é‡å¤„ç†é—®é¢˜æ–‡ä»¶ï¼ˆJSONæ ¼å¼ï¼‰")
    parser.add_argument("--save-history", action="store_true", 
                       help="ä¿å­˜å¯¹è¯å†å²")
    
    args = parser.parse_args()
    
    # åˆ›å»ºåŒ»ç–—åŠ©æ‰‹å®ä¾‹
    assistant = MedicalAssistant(args.checkpoint)
    
    # åŠ è½½æ¨¡å‹
    assistant.load_model()
    
    if args.batch:
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        assistant.batch_questions(args.batch)
    elif args.question:
        # å•æ¬¡é—®ç­”æ¨¡å¼
        print(f"ğŸ¤– åŒ»ç–—åŠ©æ‰‹å›ç­”:")
        print("=" * 50)
        response = assistant.ask_question(args.question, args.scenario, args.max_tokens)
        print(response)
        print("=" * 50)
    else:
        # äº¤äº’æ¨¡å¼
        assistant.interactive_mode()
    
    # ä¿å­˜å¯¹è¯å†å²
    if args.save_history and assistant.conversation_history:
        assistant.save_conversation()


if __name__ == "__main__":
    main()
